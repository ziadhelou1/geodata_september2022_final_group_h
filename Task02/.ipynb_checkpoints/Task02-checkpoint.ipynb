{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# geo0930: Insert time series into PostgreSQL/PostGIS and join it with the station info geodata.\n",
    "\n",
    "The main idea behind this activity is to reformat and merge time series (here we use hourly precipitation) as well as weather station information from the DWD Climate Data Center in such a way that it can be used with the **QGIS TimeManager extension**. But this time the **join** of station info geodata and time series are performed in **PostgreSQL/PostGIS** instead of Pandas and CSV file.\n",
    "\n",
    "**UPDATE: The QGIS TimeManager extension is deprecated!** Nevertheless the principle of merging (joining) static location and time varying weather data (temperature, precipitation, etc.) is the same for the **new way to handle time dependent geodata in QGIS.**\n",
    "\n",
    "Below you find the description of how to manage time dependent data with the deprecated TimeManager. This can be transferred to the new QGIS time handling.\n",
    "\n",
    "The TimeManager allows to filter an attribute table of a vector layer (e.g. points representing precipitation stations plus precipitation data) with a time stamp column. The extension limits the attribute table to the records matching the particular time stamp provided by the time manager extension (e.g. by the user moving the time slider). This selected subset of the attribute table is then used to change the sympology of the vector layer according to the variable of interest (e.g. precipitation rate).\n",
    "\n",
    "This relation created by joining station info geodata with time series is a 1:N relationship: 1 station has N measurements values. They can be distinguished by timestamp. Technically the primary key for that relation consists of the two attributes (station_id, timestamp). \n",
    "\n",
    "The final data format is a concatenation of time series together with geographic location in 2D (e.g. lat, lon). The required data format looks principly like this:\n",
    "\n",
    "\n",
    "| station_id |        name        |   lat   |   lon  |        meas_time       | prec_rate |\n",
    "|:----------:|:------------------:|:-------:|:------:|:----------------------:|:---------:|\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T08:00:00UTC |       1.5 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T09:00:00UTC |       1.7 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T10:00:00UTC |       0.1 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T08:00:00UTC |       0.8 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T09:00:00UTC |       0.4 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T10:00:00UTC |       0.0 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "\n",
    "Primary key of this example relation is (station_id, meas_time).\n",
    "\n",
    "(Table generated with https://www.tablesgenerator.com/markdown_tables)\n",
    "\n",
    "This relation was realized in an earlier activity in Pandas and saved as CSV which then was imported to QGIS and used in the TimeManager. This approach is quite brute force, because the data is highly redundent. Example: If the time series at a single station X contains 1000 values then the feature table will contain 1000 rows for that station, one feature with geometry information and measurement value for each timestamp of the time series. Neither station id, station name nor coordinates differ. The only difference are the timestamps and the associated measurement values. And all these 1000 features belonging to one station are plotted on top of each other. The TimeManager then selects from the feature table only those features which match a given timestamp. In this selection each station occurs only once. This view is a snapshot of the precipitation measurements at all stations included for a given time.\n",
    "\n",
    "This activity demonstrates an alternative approach. Instead of writing the 1:N relationship to a CSV file (which can become very large!) and importing this to QGIS the join is performed in PostGIS. The two relations (tables) involved are the station info layer with geometry column (primary key: station_id) and the table with the precipitation time series (Promary key: station_id, timestamp). The join of these tables is then stored as a view. This is a kind of virtual table. When you select from the view it looks as it where a table (in fact, it is a relation), but the information is selected and joined from the underlying tables during execution time.\n",
    "\n",
    "This stored view can be imported in QGIS as point vector layer as if it were a geodata table. It is noteworthy that this link is live connection. Any change of the data in PostGIS will be immediately updated in QGIS and vice versa!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FTP Connection\n",
    "\n",
    "* FTP: ftp://opendata.dwd.de/climate_environment/CDC/observations_germany/\n",
    "* HTTPS: https://opendata.dwd.de/climate_environment/CDC/observations_germany/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"opendata.dwd.de\"\n",
    "user   = \"anonymous\"\n",
    "passwd = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Directory Definition and Station Description Filename Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topic of interest.\n",
    "topic_dir = \"/hourly/precipitation/historical/\"\n",
    "#topic_dir = \"/annual/kl/historical/\"\n",
    "\n",
    "# This is the search pattern common to ALL station description file names \n",
    "station_desc_pattern = \"_Beschreibung_Stationen.txt\"\n",
    "\n",
    "# Below this directory tree node all climate data are stored.\n",
    "ftp_climate_data_dir = \"/climate_environment/CDC/observations_germany/climate/\"\n",
    "ftp_dir =  ftp_climate_data_dir + topic_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Create Local Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ftp_dir         = \"../data/original/DWD/\"      # Local directory to store local ftp data copies, the local data source or input data. \n",
    "local_ftp_station_dir = local_ftp_dir + topic_dir # Local directory where local station info is located\n",
    "local_ftp_ts_dir      = local_ftp_dir + topic_dir # Local directory where time series downloaded from ftp are located\n",
    "\n",
    "local_generated_dir   = \"../data/generated/DWD/\" # The generated of derived data in contrast to local_ftp_dir\n",
    "local_station_dir     = local_generated_dir + topic_dir # Derived station data, i.e. the CSV file\n",
    "local_ts_merged_dir   = local_generated_dir + topic_dir # Parallelly merged time series, wide data frame with one TS per column\n",
    "local_ts_appended_dir = local_generated_dir + topic_dir # Serially appended time series, long data frame for QGIS TimeManager Plugin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/original/DWD/\n",
      "../data/original/DWD//hourly/precipitation/historical/\n",
      "../data/original/DWD//hourly/precipitation/historical/\n",
      "\n",
      "../data/generated/DWD/\n",
      "../data/generated/DWD//hourly/precipitation/historical/\n",
      "../data/generated/DWD//hourly/precipitation/historical/\n",
      "../data/generated/DWD//hourly/precipitation/historical/\n"
     ]
    }
   ],
   "source": [
    "print(local_ftp_dir)\n",
    "print(local_ftp_station_dir)\n",
    "print(local_ftp_ts_dir)\n",
    "print()\n",
    "print(local_generated_dir)\n",
    "print(local_station_dir)\n",
    "print(local_ts_merged_dir)\n",
    "print(local_ts_appended_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(local_ftp_dir,exist_ok = True) # it does not complain if the dir already exists.\n",
    "os.makedirs(local_ftp_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ftp_ts_dir,exist_ok = True)\n",
    "\n",
    "os.makedirs(local_generated_dir,exist_ok = True)\n",
    "os.makedirs(local_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_merged_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_appended_dir,exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 Login successful.\n"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "ftp = ftplib.FTP(server)\n",
    "res = ftp.login(user=user, passwd = passwd)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = ftp.cwd(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pandas Dataframe from FTP Directory Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>BESCHREIBUNG_obsgermany_climate_hourly_precipi...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>166317</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>DESCRIPTION_obsgermany_climate_hourly_precipit...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>161348</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>RR_Stundenwerte_Beschreibung_Stationen.txt</td>\n",
       "      <td>.txt</td>\n",
       "      <td>310685</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stundenwerte_RR_00003_19950901_20110401_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>418905</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>stundenwerte_RR_00020_20040814_20211231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>456263</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id                                               name   ext  \\\n",
       "0          -1  BESCHREIBUNG_obsgermany_climate_hourly_precipi...  .pdf   \n",
       "1          -1  DESCRIPTION_obsgermany_climate_hourly_precipit...  .pdf   \n",
       "2          -1         RR_Stundenwerte_Beschreibung_Stationen.txt  .txt   \n",
       "3           3   stundenwerte_RR_00003_19950901_20110401_hist.zip  .zip   \n",
       "4          20   stundenwerte_RR_00020_20040814_20211231_hist.zip  .zip   \n",
       "\n",
       "     size type  \n",
       "0  166317    -  \n",
       "1  161348    -  \n",
       "2  310685    -  \n",
       "3  418905    -  \n",
       "4  456263    -  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_dwd import gen_df_from_ftp_dir_listing\n",
    "df_ftpdir = gen_df_from_ftp_dir_listing(ftp, ftp_dir)\n",
    "df_ftpdir.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Process the Station Description File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the txt File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dwd import grabFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station description file name:\n",
      "RR_Stundenwerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "station_fname = df_ftpdir[df_ftpdir['name'].str.contains(station_desc_pattern)][\"name\"].values[0]\n",
    "print(\"Station description file name:\\n%s\" % (station_fname))\n",
    "\n",
    "# ALternative\n",
    "#station_fname2 = df_ftpdir[df_ftpdir[\"name\"].str.match(\"^.*Beschreibung_Stationen.*txt$\")][\"name\"].values[0]\n",
    "#print(station_fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabFile(ftp, src, dest):\n",
      "FTP source: /climate_environment/CDC/observations_germany/climate//hourly/precipitation/historical/RR_Stundenwerte_Beschreibung_Stationen.txt\n",
      "Local dest:   ../data/original/DWD//hourly/precipitation/historical/RR_Stundenwerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "src = ftp_dir + station_fname\n",
    "dest = local_ftp_station_dir + station_fname\n",
    "print(\"grabFile(ftp, src, dest):\")\n",
    "print(\"FTP source: \" + src)\n",
    "print(\"Local dest:   \" + dest)\n",
    "grabFile(ftp, src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the Column Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>202</td>\n",
       "      <td>50.7827</td>\n",
       "      <td>6.0941</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2004-08-14</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>432</td>\n",
       "      <td>48.9219</td>\n",
       "      <td>9.9129</td>\n",
       "      <td>Abtsgmünd-Untergröningen</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2006-01-10</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>260</td>\n",
       "      <td>49.7175</td>\n",
       "      <td>10.9101</td>\n",
       "      <td>Adelsdorf (Kläranlage)</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2007-04-01</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>44</td>\n",
       "      <td>52.9336</td>\n",
       "      <td>8.2370</td>\n",
       "      <td>Großenkneten</td>\n",
       "      <td>Niedersachsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>325</td>\n",
       "      <td>48.9450</td>\n",
       "      <td>12.4639</td>\n",
       "      <td>Aholfing</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "3          1995-09-01 2011-04-01       202   50.7827     6.0941   \n",
       "20         2004-08-14 2022-09-20       432   48.9219     9.9129   \n",
       "29         2006-01-10 2022-09-20       260   49.7175    10.9101   \n",
       "44         2007-04-01 2022-09-20        44   52.9336     8.2370   \n",
       "46         2006-01-03 2022-09-20       325   48.9450    12.4639   \n",
       "\n",
       "                                name                state  \n",
       "station_id                                                 \n",
       "3                             Aachen  Nordrhein-Westfalen  \n",
       "20          Abtsgmünd-Untergröningen    Baden-Württemberg  \n",
       "29            Adelsdorf (Kläranlage)               Bayern  \n",
       "44                      Großenkneten        Niedersachsen  \n",
       "46                          Aholfing               Bayern  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_dwd import station_desc_txt_to_csv\n",
    "basename = os.path.splitext(station_fname)[0]\n",
    "df_stations = station_desc_txt_to_csv(local_ftp_station_dir + station_fname, local_station_dir + basename + \".csv\")\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only Stations Located in NRW and being Operational "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station_ids_selected = df_stations[df_stations['state'].str.contains(\"Nordrhein\")].index\n",
    "#station_ids_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>298</td>\n",
       "      <td>51.1143</td>\n",
       "      <td>7.8807</td>\n",
       "      <td>Attendorn-Neulisternohl</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>436</td>\n",
       "      <td>51.0148</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>Berleburg, Bad-Arfeld</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>610</td>\n",
       "      <td>50.9837</td>\n",
       "      <td>8.3683</td>\n",
       "      <td>Berleburg, Bad-Stünzel</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>23</td>\n",
       "      <td>51.8293</td>\n",
       "      <td>6.5365</td>\n",
       "      <td>Bocholt-Liedern (Wasserwerk)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>1999-03-03</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>147</td>\n",
       "      <td>50.7293</td>\n",
       "      <td>7.2040</td>\n",
       "      <td>Königswinter-Heiderhof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19462</th>\n",
       "      <td>2022-06-06</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>23</td>\n",
       "      <td>51.6093</td>\n",
       "      <td>6.3604</td>\n",
       "      <td>Sonsbeck</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19463</th>\n",
       "      <td>2022-06-06</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>37</td>\n",
       "      <td>51.3326</td>\n",
       "      <td>6.4638</td>\n",
       "      <td>Tönisvorst-St. Tönis</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19464</th>\n",
       "      <td>2022-06-06</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>34</td>\n",
       "      <td>51.4538</td>\n",
       "      <td>6.2817</td>\n",
       "      <td>Straelen/Niederrhein</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19467</th>\n",
       "      <td>2022-06-06</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>24</td>\n",
       "      <td>51.6652</td>\n",
       "      <td>6.2581</td>\n",
       "      <td>Uedem/Niederrhein</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19469</th>\n",
       "      <td>2022-06-06</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>29</td>\n",
       "      <td>51.4077</td>\n",
       "      <td>6.3243</td>\n",
       "      <td>Wachtendonk/Niederrhein</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "216        2004-10-01 2022-09-20       298   51.1143     7.8807   \n",
       "389        2009-11-01 2022-09-20       436   51.0148     8.4318   \n",
       "390        2004-07-01 2022-09-20       610   50.9837     8.3683   \n",
       "554        1995-09-01 2022-09-20        23   51.8293     6.5365   \n",
       "603        1999-03-03 2022-09-20       147   50.7293     7.2040   \n",
       "...               ...        ...       ...       ...        ...   \n",
       "19462      2022-06-06 2022-09-20        23   51.6093     6.3604   \n",
       "19463      2022-06-06 2022-09-20        37   51.3326     6.4638   \n",
       "19464      2022-06-06 2022-09-20        34   51.4538     6.2817   \n",
       "19467      2022-06-06 2022-09-20        24   51.6652     6.2581   \n",
       "19469      2022-06-06 2022-09-20        29   51.4077     6.3243   \n",
       "\n",
       "                                    name                state  \n",
       "station_id                                                     \n",
       "216              Attendorn-Neulisternohl  Nordrhein-Westfalen  \n",
       "389                Berleburg, Bad-Arfeld  Nordrhein-Westfalen  \n",
       "390               Berleburg, Bad-Stünzel  Nordrhein-Westfalen  \n",
       "554         Bocholt-Liedern (Wasserwerk)  Nordrhein-Westfalen  \n",
       "603               Königswinter-Heiderhof  Nordrhein-Westfalen  \n",
       "...                                  ...                  ...  \n",
       "19462                           Sonsbeck  Nordrhein-Westfalen  \n",
       "19463               Tönisvorst-St. Tönis  Nordrhein-Westfalen  \n",
       "19464               Straelen/Niederrhein  Nordrhein-Westfalen  \n",
       "19467                  Uedem/Niederrhein  Nordrhein-Westfalen  \n",
       "19469            Wachtendonk/Niederrhein  Nordrhein-Westfalen  \n",
       "\n",
       "[160 rows x 7 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create variable with TRUE if state is Nordrhein-Westfalen\n",
    "\n",
    "# isNRW = df_stations['state'] == \"Nordrhein-Westfalen\"\n",
    "isNRW = df_stations['state'].str.contains(\"Nordrhein\")\n",
    "\n",
    "# Create variable with TRUE if date_to is latest date (indicates operation up to now)\n",
    "isOperational = df_stations['date_to'] == df_stations.date_to.max() \n",
    "\n",
    "#isBefore1950 = df_stations['date_from'] < '1950'\n",
    "#dfNRW = df_stations[isNRW & isOperational & isBefore1950]\n",
    "\n",
    "# select on both conditions\n",
    "\n",
    "dfNRW = df_stations[isNRW & isOperational]\n",
    "\n",
    "#print(\"Number of stations in NRW: \\n\", dfNRW.count())\n",
    "dfNRW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geopandas - Create a Geo Data Frame\n",
    "\n",
    "A Geopandas geo data frame is a Pandas data frame enriched with an additional geometry column. Each row in the data frame becomes a location information. Thus a geo-df contains geometry and attributes, i.e. full features. The geo-df is self-contained and complete. It can be easily saved in different vectore file formats, i.e. shapefile or geopackage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue: some `pyproj` installations with wrong `PROJ_LIB` environment variable value \n",
    "\n",
    "Problem:\n",
    "\n",
    "```\n",
    "C:\\Users\\me\\Anaconda3\\envs\\geo\\lib\\site-packages\\pyproj\\__init__.py:89: UserWarning: pyproj unable to set database path.\n",
    "  _pyproj_global_context_initialize()\n",
    "[...]\n",
    "CRSError: Invalid projection: epsg:4326: (Internal Proj Error: proj_create: no database context specified)\n",
    "```\n",
    "\n",
    "\n",
    "This problem seems to occur on Windows when using the OSGeo4W installer. The environment variable must point to a user specific directory and according to the activated conda environment, e.g. `PROJ_LIB=C:\\Users\\<username>\\Anaconda3\\envs\\geo\\Library\\share\\proj` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONDA_PREFIX: C:\\Users\\HP\\anaconda3\n",
      "New env var value: \n",
      "PROJ_LIB=C:\\Users\\HP\\anaconda3\\Library\\share\\proj\n",
      "pyproj.datadir.get_data_dir() -> C:\\Users\\HP\\anaconda3\\Library\\share\\proj\n"
     ]
    }
   ],
   "source": [
    "# Correct wrong environment variable value occurring when using OSGeo4W installer\n",
    "\n",
    "import os\n",
    "#proj_lib = os.environ['proj_lib']\n",
    "#print(proj_lib)\n",
    "#-> C:\\OSGeo4W64\\share\\proj (wrong!)\n",
    "\n",
    "conda_prefix = os.environ['conda_prefix']\n",
    "print(f\"CONDA_PREFIX: {conda_prefix:s}\")\n",
    "os.environ['proj_lib'] = conda_prefix + r\"\\Library\\share\\proj\"\n",
    "proj_lib = os.environ['proj_lib']\n",
    "print(f\"New env var value: \\nPROJ_LIB={proj_lib:s}\")\n",
    "#-> C:\\Users\\me\\Anaconda3\\envs\\geo\\Library\\share\\proj (correct!)\n",
    "\n",
    "# Now pyproj should work\n",
    "import pyproj\n",
    "print(f\"pyproj.datadir.get_data_dir() -> {pyproj.datadir.get_data_dir():s}\") \n",
    "\n",
    "# Now geopandas (it uses pyproj) should work:\n",
    "# import geopandas as gpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>298</td>\n",
       "      <td>51.1143</td>\n",
       "      <td>7.8807</td>\n",
       "      <td>Attendorn-Neulisternohl</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (7.88070 51.11430)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>436</td>\n",
       "      <td>51.0148</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>Berleburg, Bad-Arfeld</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (8.43180 51.01480)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>610</td>\n",
       "      <td>50.9837</td>\n",
       "      <td>8.3683</td>\n",
       "      <td>Berleburg, Bad-Stünzel</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (8.36830 50.98370)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>23</td>\n",
       "      <td>51.8293</td>\n",
       "      <td>6.5365</td>\n",
       "      <td>Bocholt-Liedern (Wasserwerk)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (6.53650 51.82930)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>1999-03-03</td>\n",
       "      <td>2022-09-20</td>\n",
       "      <td>147</td>\n",
       "      <td>50.7293</td>\n",
       "      <td>7.2040</td>\n",
       "      <td>Königswinter-Heiderhof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (7.20400 50.72930)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "216        2004-10-01 2022-09-20       298   51.1143     7.8807   \n",
       "389        2009-11-01 2022-09-20       436   51.0148     8.4318   \n",
       "390        2004-07-01 2022-09-20       610   50.9837     8.3683   \n",
       "554        1995-09-01 2022-09-20        23   51.8293     6.5365   \n",
       "603        1999-03-03 2022-09-20       147   50.7293     7.2040   \n",
       "\n",
       "                                    name                state  \\\n",
       "station_id                                                      \n",
       "216              Attendorn-Neulisternohl  Nordrhein-Westfalen   \n",
       "389                Berleburg, Bad-Arfeld  Nordrhein-Westfalen   \n",
       "390               Berleburg, Bad-Stünzel  Nordrhein-Westfalen   \n",
       "554         Bocholt-Liedern (Wasserwerk)  Nordrhein-Westfalen   \n",
       "603               Königswinter-Heiderhof  Nordrhein-Westfalen   \n",
       "\n",
       "                            geometry  \n",
       "station_id                            \n",
       "216         POINT (7.88070 51.11430)  \n",
       "389         POINT (8.43180 51.01480)  \n",
       "390         POINT (8.36830 50.98370)  \n",
       "554         POINT (6.53650 51.82930)  \n",
       "603         POINT (7.20400 50.72930)  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import fiona\n",
    "from pyproj import CRS\n",
    "\n",
    "#df = pd.read_csv('data.csv')\n",
    "df = dfNRW\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df.longitude, df.latitude)]\n",
    "crs = CRS(\"epsg:4326\") #http://www.spatialreference.org/ref/epsg/2263/\n",
    "stations_gdf = GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "\n",
    "stations_gdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the PostGIS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection URL:  postgresql://geo_master:xxxxxx@localhost:5432/geo\n"
     ]
    }
   ],
   "source": [
    "# PostgreSQL connection parameters -> create connection string (URL) \n",
    "\n",
    "param_dic = {\n",
    "  \"user\" : \"geo_master\",\n",
    "  \"pw\"   : \"xxxxxx\",\n",
    "  \"host\" : \"localhost\",\n",
    "  \"db\"   : \"geo\"\n",
    "}\n",
    "\n",
    "# https://www.w3schools.com/python/ref_string_format.asp\n",
    "template = \"postgresql://{user}:{pw}@{host}:5432/{db}\"\n",
    "\n",
    "db_connection_url = template.format(**param_dic)\n",
    "print(\"Connection URL: \", db_connection_url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Geopandas Data Frame directly into PostGIS Database\n",
    "\n",
    "* https://geopandas.readthedocs.io/en/latest/docs/reference/api/geopandas.GeoDataFrame.to_postgis.html\n",
    "* https://docs.sqlalchemy.org/en/13/core/types.html\n",
    "* https://www.postgresqltutorial.com/postgresql-primary-key/\n",
    "* https://www.postgresql.org/docs/13/sql-altertable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "(psycopg2.errors.DependentObjectsStillExist) cannot drop table dwd.stations because other objects depend on it\nDETAIL:  view dwd.v_stations_prec depends on table dwd.stations\nHINT:  Use DROP ... CASCADE to drop the dependent objects too.\n\n[SQL: \nDROP TABLE dwd.stations]\n(Background on this error at: https://sqlalche.me/e/14/2j85)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDependentObjectsStillExist\u001b[0m                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1808\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1808\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\default.py:732\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 732\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mDependentObjectsStillExist\u001b[0m: cannot drop table dwd.stations because other objects depend on it\nDETAIL:  view dwd.v_stations_prec depends on table dwd.stations\nHINT:  Use DROP ... CASCADE to drop the dependent objects too.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [91]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(db_connection_url)\n\u001b[0;32m      9\u001b[0m dtypes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstation_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: Numeric (\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maltitude\u001b[39m\u001b[38;5;124m\"\u001b[39m : REAL, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_from\u001b[39m\u001b[38;5;124m\"\u001b[39m : Date, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_to\u001b[39m\u001b[38;5;124m\"\u001b[39m: Date, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m : REAL, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m : REAL}\n\u001b[1;32m---> 11\u001b[0m \u001b[43mstations_gdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_postgis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdwd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstation_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m engine\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malter table dwd.stations add primary key (station_id)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\geopandas\\geodataframe.py:1960\u001b[0m, in \u001b[0;36mGeoDataFrame.to_postgis\u001b[1;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype)\u001b[0m\n\u001b[0;32m   1900\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_postgis\u001b[39m(\n\u001b[0;32m   1901\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1902\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1909\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1910\u001b[0m ):\n\u001b[0;32m   1911\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;124;03m    Upload GeoDataFrame into PostGIS database.\u001b[39;00m\n\u001b[0;32m   1913\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1958\u001b[0m \n\u001b[0;32m   1959\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1960\u001b[0m     \u001b[43mgeopandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_postgis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[0;32m   1962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\geopandas\\io\\sql.py:434\u001b[0m, in \u001b[0;36m_write_postgis\u001b[1;34m(gdf, name, con, schema, if_exists, index, index_label, chunksize, dtype)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _get_conn(con) \u001b[38;5;28;01mas\u001b[39;00m connection:\n\u001b[1;32m--> 434\u001b[0m     \u001b[43mgdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_psql_insert_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2951\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[1;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   2794\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2795\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[0;32m   2796\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2947\u001b[0m \u001b[38;5;124;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[0;32m   2948\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa:E501\u001b[39;00m\n\u001b[0;32m   2949\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[1;32m-> 2951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2952\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2962\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:697\u001b[0m, in \u001b[0;36mto_sql\u001b[1;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(frame, DataFrame):\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    694\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument should be either a Series or a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    695\u001b[0m     )\n\u001b[1;32m--> 697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mto_sql(\n\u001b[0;32m    698\u001b[0m     frame,\n\u001b[0;32m    699\u001b[0m     name,\n\u001b[0;32m    700\u001b[0m     if_exists\u001b[38;5;241m=\u001b[39mif_exists,\n\u001b[0;32m    701\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    702\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m    703\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m    704\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    705\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    706\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    707\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengine_kwargs,\n\u001b[0;32m    709\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:1729\u001b[0m, in \u001b[0;36mSQLDatabase.to_sql\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;124;03m    Any additional kwargs are passed to the engine.\u001b[39;00m\n\u001b[0;32m   1726\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1727\u001b[0m sql_engine \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m-> 1729\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1739\u001b[0m total_inserted \u001b[38;5;241m=\u001b[39m sql_engine\u001b[38;5;241m.\u001b[39minsert_records(\n\u001b[0;32m   1740\u001b[0m     table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[0;32m   1741\u001b[0m     con\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnectable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1748\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengine_kwargs,\n\u001b[0;32m   1749\u001b[0m )\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_case_sensitive(name\u001b[38;5;241m=\u001b[39mname, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:1628\u001b[0m, in \u001b[0;36mSQLDatabase.prep_table\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, dtype)\u001b[0m\n\u001b[0;32m   1616\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe type of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a SQLAlchemy type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1618\u001b[0m table \u001b[38;5;241m=\u001b[39m SQLTable(\n\u001b[0;32m   1619\u001b[0m     name,\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1626\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1627\u001b[0m )\n\u001b[1;32m-> 1628\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:835\u001b[0m, in \u001b[0;36mSQLTable.create\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mif_exists \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 835\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpd_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_create()\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mif_exists \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:1788\u001b[0m, in \u001b[0;36mSQLDatabase.drop_table\u001b[1;34m(self, table_name, schema)\u001b[0m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_table(table_name, schema):\n\u001b[0;32m   1787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mreflect(bind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnectable, only\u001b[38;5;241m=\u001b[39m[table_name], schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m-> 1788\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnectable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\schema.py:967\u001b[0m, in \u001b[0;36mTable.drop\u001b[1;34m(self, bind, checkfirst)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bind \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    966\u001b[0m     bind \u001b[38;5;241m=\u001b[39m _bind_or_error(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 967\u001b[0m \u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_ddl_visitor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSchemaDropper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckfirst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py:2119\u001b[0m, in \u001b[0;36mConnection._run_ddl_visitor\u001b[1;34m(self, visitorcallable, element, **kwargs)\u001b[0m\n\u001b[0;32m   2112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_ddl_visitor\u001b[39m(\u001b[38;5;28mself\u001b[39m, visitorcallable, element, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2113\u001b[0m     \u001b[38;5;124;03m\"\"\"run a DDL visitor.\u001b[39;00m\n\u001b[0;32m   2114\u001b[0m \n\u001b[0;32m   2115\u001b[0m \u001b[38;5;124;03m    This method is only here so that the MockConnection can change the\u001b[39;00m\n\u001b[0;32m   2116\u001b[0m \u001b[38;5;124;03m    options given to the visitor so that \"checkfirst\" is skipped.\u001b[39;00m\n\u001b[0;32m   2117\u001b[0m \n\u001b[0;32m   2118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2119\u001b[0m     \u001b[43mvisitorcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraverse_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\visitors.py:524\u001b[0m, in \u001b[0;36mExternalTraversal.traverse_single\u001b[1;34m(self, obj, **kw)\u001b[0m\n\u001b[0;32m    522\u001b[0m meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m obj\u001b[38;5;241m.\u001b[39m__visit_name__, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meth:\n\u001b[1;32m--> 524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\ddl.py:1100\u001b[0m, in \u001b[0;36mSchemaDropper.visit_table\u001b[1;34m(self, table, drop_ok, _is_metadata_operation, _ignore_sequences)\u001b[0m\n\u001b[0;32m   1090\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m table\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mbefore_drop(\n\u001b[0;32m   1093\u001b[0m     table,\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1097\u001b[0m     _is_metadata_operation\u001b[38;5;241m=\u001b[39m_is_metadata_operation,\n\u001b[0;32m   1098\u001b[0m )\n\u001b[1;32m-> 1100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDropTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;66;03m# traverse client side defaults which may refer to server-side\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# sequences. noting that some of these client side defaults may also be\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;66;03m# set up as server side defaults (see https://docs.sqlalchemy.org/en/\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;66;03m# latest/core/defaults.html#associating-a-sequence-as-the-server-side-\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# default), so have to be dropped after the table is dropped.\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1295\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[0;32m   1292\u001b[0m         exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement), replace_context\u001b[38;5;241m=\u001b[39merr\n\u001b[0;32m   1293\u001b[0m     )\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_EMPTY_EXECUTION_OPTS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\ddl.py:80\u001b[0m, in \u001b[0;36mDDLElement._execute_on_connection\u001b[1;34m(self, connection, multiparams, params, execution_options)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_on_connection\u001b[39m(\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m, connection, multiparams, params, execution_options\n\u001b[0;32m     79\u001b[0m ):\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_ddl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1387\u001b[0m, in \u001b[0;36mConnection._execute_ddl\u001b[1;34m(self, ddl, multiparams, params, execution_options)\u001b[0m\n\u001b[0;32m   1382\u001b[0m dialect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\n\u001b[0;32m   1384\u001b[0m compiled \u001b[38;5;241m=\u001b[39m ddl\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1385\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect, schema_translate_map\u001b[38;5;241m=\u001b[39mschema_translate_map\n\u001b[0;32m   1386\u001b[0m )\n\u001b[1;32m-> 1387\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_ddl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[0;32m   1396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[0;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1398\u001b[0m         ddl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1402\u001b[0m         ret,\n\u001b[0;32m   1403\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1851\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1848\u001b[0m             branched\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1851\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py:2032\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[0;32m   2030\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(newraise, with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m], from_\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m   2031\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[1;32m-> 2032\u001b[0m     \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2033\u001b[0m \u001b[43m        \u001b[49m\u001b[43msqlalchemy_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_traceback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\n\u001b[0;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2035\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2036\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(exc_info[\u001b[38;5;241m1\u001b[39m], with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\util\\compat.py:207\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    204\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m replace_context\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1808\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1806\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1808\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[0;32m   1814\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1815\u001b[0m         cursor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1819\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[0;32m   1820\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\default.py:732\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 732\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: (psycopg2.errors.DependentObjectsStillExist) cannot drop table dwd.stations because other objects depend on it\nDETAIL:  view dwd.v_stations_prec depends on table dwd.stations\nHINT:  Use DROP ... CASCADE to drop the dependent objects too.\n\n[SQL: \nDROP TABLE dwd.stations]\n(Background on this error at: https://sqlalche.me/e/14/2j85)"
     ]
    }
   ],
   "source": [
    "# https://geopandas.readthedocs.io/en/latest/docs/reference/api/geopandas.GeoDataFrame.to_postgis.html\n",
    "# https://docs.sqlalchemy.org/en/13/core/types.html\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import Numeric, Float, Date, REAL\n",
    "\n",
    "engine = create_engine(db_connection_url)\n",
    "\n",
    "dtypes = {\"station_id\": Numeric (6,0), \"altitude\" : REAL, \"date_from\" : Date, \"date_to\": Date, \"longitude\" : REAL, \"latitude\" : REAL}\n",
    "\n",
    "stations_gdf.to_postgis(name=\"stations\", schema=\"dwd\", if_exists = \"replace\", index = \"station_id\", index_label=True, con=engine, dtype = dtypes)\n",
    "\n",
    "engine.execute('alter table dwd.stations add primary key (station_id)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Process the Time Series Zip Archives\n",
    "\n",
    "Extract the product file (txt file containing several time series for different variables) from an archive, extract the relevant time series from the product file, limit the time series interval if needed and append it to a dataframe. Finally insert the dataframe to the PostGIS database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe with TS Zip Files from FTP Directory Listing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stundenwerte_RR_00003_19950901_20110401_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>418905</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stundenwerte_RR_00020_20040814_20211231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>456263</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>stundenwerte_RR_00044_20070401_20211231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>378416</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>stundenwerte_RR_00053_20051001_20211231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>409591</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>stundenwerte_RR_00071_20041022_20200101_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>402406</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        name   ext    size  \\\n",
       "station_id                                                                   \n",
       "3           stundenwerte_RR_00003_19950901_20110401_hist.zip  .zip  418905   \n",
       "20          stundenwerte_RR_00020_20040814_20211231_hist.zip  .zip  456263   \n",
       "44          stundenwerte_RR_00044_20070401_20211231_hist.zip  .zip  378416   \n",
       "53          stundenwerte_RR_00053_20051001_20211231_hist.zip  .zip  409591   \n",
       "71          stundenwerte_RR_00071_20041022_20200101_hist.zip  .zip  402406   \n",
       "\n",
       "           type  \n",
       "station_id       \n",
       "3             -  \n",
       "20            -  \n",
       "44            -  \n",
       "53            -  \n",
       "71            -  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_ftpdir[\"ext\"]==\".zip\"\n",
    "df_zips = df_ftpdir[df_ftpdir[\"ext\"]==\".zip\"]\n",
    "df_zips.set_index(\"station_id\", inplace = True)\n",
    "df_zips.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TS Data from FTP Server\n",
    "\n",
    "**Problem:** Not all stations listed in the station description file are associated with a time series (zip file)! The stations in the description file and the set of stations whoch are TS data provided for (zip files) do not match perfectly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stundenwerte_RR_00216_20041001_20211231_hist.zip\n",
      "stundenwerte_RR_00389_20091101_20211231_hist.zip\n",
      "stundenwerte_RR_00390_20040701_20211231_hist.zip\n",
      "stundenwerte_RR_00554_19950901_20211231_hist.zip\n",
      "stundenwerte_RR_00603_19990303_20211231_hist.zip\n",
      "stundenwerte_RR_00613_20041101_20211231_hist.zip\n",
      "stundenwerte_RR_00617_20040601_20211231_hist.zip\n",
      "stundenwerte_RR_00644_20050101_20211231_hist.zip\n",
      "stundenwerte_RR_00796_20041101_20211231_hist.zip\n",
      "stundenwerte_RR_00871_20050801_20211231_hist.zip\n",
      "stundenwerte_RR_00902_20061001_20211231_hist.zip\n",
      "stundenwerte_RR_00934_20041001_20211231_hist.zip\n",
      "stundenwerte_RR_00989_20050201_20211231_hist.zip\n",
      "stundenwerte_RR_01024_20060801_20211231_hist.zip\n",
      "stundenwerte_RR_01046_20041001_20211231_hist.zip\n",
      "stundenwerte_RR_01078_19950901_20211231_hist.zip\n",
      "stundenwerte_RR_01241_20061201_20211231_hist.zip\n",
      "stundenwerte_RR_01246_20150801_20211231_hist.zip\n",
      "stundenwerte_RR_01300_20040601_20211231_hist.zip\n",
      "stundenwerte_RR_01303_19950901_20211231_hist.zip\n",
      "stundenwerte_RR_01327_20040801_20211231_hist.zip\n",
      "stundenwerte_RR_01590_20030701_20211231_hist.zip\n",
      "stundenwerte_RR_01595_20121001_20211231_hist.zip\n",
      "stundenwerte_RR_01766_19950901_20211231_hist.zip\n",
      "stundenwerte_RR_02027_20060601_20211231_hist.zip\n",
      "stundenwerte_RR_02110_20030105_20211231_hist.zip\n",
      "stundenwerte_RR_02254_20050601_20211231_hist.zip\n",
      "stundenwerte_RR_02473_20041201_20211231_hist.zip\n",
      "stundenwerte_RR_02483_19951012_20211231_hist.zip\n",
      "stundenwerte_RR_02497_20040801_20211231_hist.zip\n",
      "stundenwerte_RR_02629_20040701_20211231_hist.zip\n",
      "stundenwerte_RR_02667_19950901_20211231_hist.zip\n",
      "stundenwerte_RR_02810_20061201_20211231_hist.zip\n",
      "stundenwerte_RR_02947_20061001_20211231_hist.zip\n",
      "stundenwerte_RR_02968_20081201_20211231_hist.zip\n",
      "stundenwerte_RR_02999_20040701_20211231_hist.zip\n",
      "stundenwerte_RR_03028_19950901_20211231_hist.zip\n",
      "stundenwerte_RR_03031_20040701_20211231_hist.zip\n",
      "stundenwerte_RR_03081_20071201_20211231_hist.zip\n",
      "stundenwerte_RR_03098_19950901_20211231_hist.zip\n",
      "stundenwerte_RR_03215_20070601_20211231_hist.zip\n",
      "stundenwerte_RR_03321_20050701_20211231_hist.zip\n",
      "WARNING: TS file for key 3328 not found in FTP directory.\n",
      "stundenwerte_RR_03339_20060901_20211231_hist.zip\n",
      "stundenwerte_RR_03499_20060701_20211231_hist.zip\n",
      "stundenwerte_RR_03540_20041101_20211231_hist.zip\n",
      "stundenwerte_RR_03591_20040601_20211231_hist.zip\n",
      "stundenwerte_RR_03795_20041201_20211231_hist.zip\n",
      "stundenwerte_RR_03913_20040701_20211231_hist.zip\n",
      "stundenwerte_RR_04063_20030701_20211231_hist.zip\n",
      "stundenwerte_RR_04127_20050101_20211231_hist.zip\n",
      "stundenwerte_RR_04150_20051201_20211231_hist.zip\n",
      "stundenwerte_RR_04313_20040801_20211231_hist.zip\n",
      "stundenwerte_RR_04368_20041001_20211231_hist.zip\n",
      "stundenwerte_RR_04371_19950901_20211231_hist.zip\n",
      "stundenwerte_RR_04400_20040801_20211231_hist.zip\n",
      "stundenwerte_RR_04488_20060801_20211231_hist.zip\n",
      "stundenwerte_RR_04741_20041201_20211231_hist.zip\n",
      "stundenwerte_RR_04849_20050601_20211231_hist.zip\n",
      "stundenwerte_RR_05064_20041201_20211231_hist.zip\n",
      "stundenwerte_RR_05347_19950901_20211231_hist.zip\n",
      "stundenwerte_RR_05360_20070701_20211231_hist.zip\n",
      "stundenwerte_RR_05480_20030910_20211231_hist.zip\n",
      "stundenwerte_RR_05513_20050901_20211231_hist.zip\n",
      "stundenwerte_RR_05619_20041201_20211231_hist.zip\n",
      "stundenwerte_RR_05699_20041101_20211231_hist.zip\n",
      "stundenwerte_RR_05717_20060901_20211231_hist.zip\n",
      "stundenwerte_RR_05733_20050501_20211231_hist.zip\n",
      "WARNING: TS file for key 6041 not found in FTP directory.\n",
      "WARNING: TS file for key 6042 not found in FTP directory.\n",
      "WARNING: TS file for key 6043 not found in FTP directory.\n",
      "WARNING: TS file for key 6044 not found in FTP directory.\n",
      "WARNING: TS file for key 6045 not found in FTP directory.\n",
      "WARNING: TS file for key 6046 not found in FTP directory.\n",
      "WARNING: TS file for key 6047 not found in FTP directory.\n",
      "WARNING: TS file for key 6048 not found in FTP directory.\n",
      "WARNING: TS file for key 6050 not found in FTP directory.\n",
      "WARNING: TS file for key 6051 not found in FTP directory.\n",
      "WARNING: TS file for key 6052 not found in FTP directory.\n",
      "WARNING: TS file for key 6053 not found in FTP directory.\n",
      "WARNING: TS file for key 6054 not found in FTP directory.\n",
      "WARNING: TS file for key 6055 not found in FTP directory.\n",
      "WARNING: TS file for key 6057 not found in FTP directory.\n",
      "WARNING: TS file for key 6058 not found in FTP directory.\n",
      "WARNING: TS file for key 6059 not found in FTP directory.\n",
      "WARNING: TS file for key 6060 not found in FTP directory.\n",
      "WARNING: TS file for key 6061 not found in FTP directory.\n",
      "WARNING: TS file for key 6064 not found in FTP directory.\n",
      "WARNING: TS file for key 6067 not found in FTP directory.\n",
      "stundenwerte_RR_06197_20001013_20211231_hist.zip\n",
      "stundenwerte_RR_06264_20040601_20211231_hist.zip\n",
      "stundenwerte_RR_06313_20041201_20211231_hist.zip\n",
      "stundenwerte_RR_06337_20040801_20211231_hist.zip\n",
      "stundenwerte_RR_07106_20060901_20211231_hist.zip\n",
      "stundenwerte_RR_07330_20051001_20211231_hist.zip\n",
      "stundenwerte_RR_07344_20060601_20211231_hist.zip\n",
      "stundenwerte_RR_07374_20060301_20211231_hist.zip\n",
      "stundenwerte_RR_07378_20060701_20211231_hist.zip\n",
      "stundenwerte_RR_13669_20070901_20211231_hist.zip\n",
      "stundenwerte_RR_13670_20070601_20211231_hist.zip\n",
      "stundenwerte_RR_13671_20071201_20211231_hist.zip\n",
      "stundenwerte_RR_13696_20071201_20211231_hist.zip\n",
      "stundenwerte_RR_13700_20080501_20211231_hist.zip\n",
      "stundenwerte_RR_13713_20071101_20211231_hist.zip\n",
      "WARNING: TS file for key 14096 not found in FTP directory.\n",
      "WARNING: TS file for key 14142 not found in FTP directory.\n",
      "WARNING: TS file for key 14143 not found in FTP directory.\n",
      "WARNING: TS file for key 14144 not found in FTP directory.\n",
      "WARNING: TS file for key 14145 not found in FTP directory.\n",
      "WARNING: TS file for key 14146 not found in FTP directory.\n",
      "WARNING: TS file for key 14147 not found in FTP directory.\n",
      "WARNING: TS file for key 14148 not found in FTP directory.\n",
      "WARNING: TS file for key 14149 not found in FTP directory.\n",
      "WARNING: TS file for key 14150 not found in FTP directory.\n",
      "WARNING: TS file for key 14151 not found in FTP directory.\n",
      "WARNING: TS file for key 14152 not found in FTP directory.\n",
      "WARNING: TS file for key 14153 not found in FTP directory.\n",
      "WARNING: TS file for key 14154 not found in FTP directory.\n",
      "WARNING: TS file for key 14155 not found in FTP directory.\n",
      "WARNING: TS file for key 14156 not found in FTP directory.\n",
      "WARNING: TS file for key 14158 not found in FTP directory.\n",
      "WARNING: TS file for key 14159 not found in FTP directory.\n",
      "WARNING: TS file for key 14164 not found in FTP directory.\n",
      "WARNING: TS file for key 14165 not found in FTP directory.\n",
      "WARNING: TS file for key 14166 not found in FTP directory.\n",
      "WARNING: TS file for key 14167 not found in FTP directory.\n",
      "WARNING: TS file for key 14168 not found in FTP directory.\n",
      "WARNING: TS file for key 14169 not found in FTP directory.\n",
      "WARNING: TS file for key 14170 not found in FTP directory.\n",
      "WARNING: TS file for key 14171 not found in FTP directory.\n",
      "WARNING: TS file for key 14172 not found in FTP directory.\n",
      "WARNING: TS file for key 14173 not found in FTP directory.\n",
      "WARNING: TS file for key 14174 not found in FTP directory.\n",
      "WARNING: TS file for key 14175 not found in FTP directory.\n",
      "WARNING: TS file for key 14176 not found in FTP directory.\n",
      "WARNING: TS file for key 14177 not found in FTP directory.\n",
      "WARNING: TS file for key 14178 not found in FTP directory.\n",
      "WARNING: TS file for key 14179 not found in FTP directory.\n",
      "WARNING: TS file for key 14180 not found in FTP directory.\n",
      "WARNING: TS file for key 14181 not found in FTP directory.\n",
      "WARNING: TS file for key 14182 not found in FTP directory.\n",
      "WARNING: TS file for key 14183 not found in FTP directory.\n",
      "WARNING: TS file for key 14184 not found in FTP directory.\n",
      "WARNING: TS file for key 14185 not found in FTP directory.\n",
      "WARNING: TS file for key 14186 not found in FTP directory.\n",
      "stundenwerte_RR_15000_20110401_20211231_hist.zip\n",
      "WARNING: TS file for key 19451 not found in FTP directory.\n",
      "WARNING: TS file for key 19452 not found in FTP directory.\n",
      "WARNING: TS file for key 19454 not found in FTP directory.\n",
      "WARNING: TS file for key 19455 not found in FTP directory.\n",
      "WARNING: TS file for key 19456 not found in FTP directory.\n",
      "WARNING: TS file for key 19457 not found in FTP directory.\n",
      "WARNING: TS file for key 19459 not found in FTP directory.\n",
      "WARNING: TS file for key 19460 not found in FTP directory.\n",
      "WARNING: TS file for key 19461 not found in FTP directory.\n",
      "WARNING: TS file for key 19462 not found in FTP directory.\n",
      "WARNING: TS file for key 19463 not found in FTP directory.\n",
      "WARNING: TS file for key 19464 not found in FTP directory.\n",
      "WARNING: TS file for key 19467 not found in FTP directory.\n",
      "WARNING: TS file for key 19469 not found in FTP directory.\n"
     ]
    }
   ],
   "source": [
    "# Add the names of the actually downloaded zip files to this list. \n",
    "local_zip_list = []\n",
    "\n",
    "# SHORTENED FOR TESTING!\n",
    "#station_ids_selected = list(dfNRW.index)[:2]\n",
    "station_ids_selected = list(dfNRW.index)\n",
    "\n",
    "for station_id in station_ids_selected:\n",
    "    try:\n",
    "        fname = df_zips[\"name\"][station_id]\n",
    "        print(fname)\n",
    "        grabFile(ftp, ftp_dir + fname, local_ftp_ts_dir + fname)\n",
    "        local_zip_list.append(fname)\n",
    "    except:\n",
    "        print(\"WARNING: TS file for key %d not found in FTP directory.\" % station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_zip_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Write the time series to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from my_dwd import prec_ts_to_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code is not necessary! It just produces a large CSV file in your folder for testing!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE EXECUTION DEACTIVATED! Change it to True if you want to run it.\n",
    "#if True:\n",
    "#if False:\n",
    "    # Produce CSV with sequentially appended time series, ca. 120 MB!\n",
    "#if True:\n",
    "csvfname = \"prec_ts_appended_3_cols.csv\"\n",
    "\n",
    "first = True\n",
    "\n",
    "for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        print(\"Zip archive: \" + ffname)\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            # read the time series data from the file starting with \"produkt\"\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "            print(\"Extract product file: %s\" % prodfilename)\n",
    "            print()\n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = prec_ts_to_df(myfile)[[\"stations_id\",\"r1\"]]\n",
    "                # df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "                dftmp.rename(columns={'stations_id': 'station_id', 'r1': 'val', 'mess_datum': 'ts'}, inplace = True)\n",
    "                dftmp.rename_axis('ts', inplace = True)\n",
    "                # dftmp.to_csv(f, header=f.tell()==0)\n",
    "                if (first):\n",
    "                    first = False\n",
    "                    dftmp.to_csv(csvfname, mode = \"w\", header = True)\n",
    "                else:\n",
    "                    dftmp.to_csv(csvfname, mode = \"a\", header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The database writer (SQL)! ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first = True\n",
    "\n",
    "dtypes = {\"station_id\": Numeric(6,0), \"val\" : REAL}\n",
    "\n",
    "#for elt in local_zip_list[0:1]:\n",
    "for elt in local_zip_list:\n",
    "    ffname = local_ftp_ts_dir + elt\n",
    "    #print(\"Zip archive: \" + ffname)\n",
    "    with ZipFile(ffname) as myzip:\n",
    "        # read the time series data from the file starting with \"produkt\"\n",
    "        prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "        print(\"Extract product file: %s\" % prodfilename)\n",
    "        # print()\n",
    "        with myzip.open(prodfilename) as myfile:\n",
    "            dftmp = prec_ts_to_df(myfile)[[\"stations_id\",\"r1\"]]\n",
    "            # df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "            dftmp.rename(columns={'stations_id': 'station_id', 'r1': 'val', 'mess_datum': 'ts'}, inplace = True)\n",
    "            dftmp.rename_axis('ts', inplace = True)\n",
    "            # dftmp.to_csv(f, header=f.tell()==0)\n",
    "            if (first):\n",
    "                first = False\n",
    "                # dftmp.to_csv(csvfname, mode = \"w\", header = False)\n",
    "                dftmp.to_sql(name=\"prec\", schema=\"dwd\", if_exists = \"replace\", index = [\"ts\"], index_label=True, con=engine, dtype=dtypes)\n",
    "            else:\n",
    "                # dftmp.to_csv(csvfname, mode = \"a\", header = False)\n",
    "                dftmp.to_sql(name=\"prec\", schema=\"dwd\", if_exists = \"append\",  index = [\"ts\"], index_label=True, con=engine, dtype=dtypes)\n",
    "\n",
    "# After insert completed: ceate index\n",
    "print(\"create index\")\n",
    "engine.execute(\"ALTER TABLE dwd.prec ADD PRIMARY KEY (ts, station_id)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create View joining static station info with the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute( \\\n",
    "\"\"\"\n",
    "CREATE OR REPLACE VIEW dwd.v_stations_prec \n",
    "as (select t1.station_id, t2.ts, t2.val, t1.geometry \n",
    "from dwd.stations t1, dwd.prec t2 \n",
    "where t2.ts between '2021-07-14T00:00:00UTC' and '2021-07-15T00:00:00UTC'\n",
    "and t1.station_id = t2.station_id)\n",
    "\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some checks ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql://geo_master:xxxxxx@localhost/geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select max(ts) from dwd.v_stations_prec \n",
    "#select * from dwd.v_stations_prec where ts between '2022-02-15 23:00:00+01:00' and '2022-02-16 00:00:00+01:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select count(*) from dwd.v_stations_prec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which Python modules are loaded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield val.__name__\n",
    "list(imports())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
